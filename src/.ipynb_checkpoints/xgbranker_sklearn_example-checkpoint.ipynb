{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangweiping/opt/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import ltr_dataset_util\n",
    "from sklearn import metrics   # Additional scklearn functions\n",
    "from sklearn.model_selection import GroupKFold, GridSearchCV\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "config_str = \"\"\n",
    "with open(r'./config.yml', 'r') as config_reader:\n",
    "    config_str = config_reader.read()\n",
    "config_yml = yaml.load(config_str)\n",
    "data_info_yml = config_yml[u'data_info']\n",
    "include_col_names = data_info_yml[u'include_col_names']\n",
    "fname = data_info_yml[u'data_file_path']\n",
    "\n",
    "x, y, g = ltr_dataset_util.load_train_data(fname, u'rate', groupkey=u'queryid', include_col_names=include_col_names)\n",
    "\n",
    "X=x\n",
    "X[X==False] = 0\n",
    "X[X==True] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(categorical_features=[5], sparse=False)\n",
    "X_code = ohe.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group_X = np.c_[g, X_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_dcg(sorted_vec, at):\n",
    "    import math\n",
    "    ranking = [t[1] for t in sorted_vec[0:at]]\n",
    "    # print ranking\n",
    "    dcg_ = sum([(2**r - 1) / math.log(i + 2, 2) for i, r in enumerate(ranking)])\n",
    "    return dcg_\n",
    "\n",
    "\n",
    "def calc_ndcg(vec, at):\n",
    "    sorted_vec = sorted(vec, key=lambda t: t[1], reverse=True)\n",
    "    ideal_dcg = calc_dcg(sorted_vec, at)\n",
    "    sorted_vec = sorted(vec, key=lambda t: t[0], reverse=True)\n",
    "    cur_dcg = calc_dcg(sorted_vec, at)\n",
    "    if ideal_dcg == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return cur_dcg / ideal_dcg\n",
    "\n",
    "\n",
    "def ndcg_scoring(estimator, X, y, sample_weight=None):\n",
    "    preds = estimator.predict_proba(X)\n",
    "    # print len(preds)\n",
    "    # print len(y)\n",
    "    # print len(X)\n",
    "    at = 3\n",
    "    ndcg_sum = 0\n",
    "    ndcg_num = 0\n",
    "    vec = []\n",
    "    pre_qid = -1\n",
    "    for i in range(len(y)):\n",
    "        qid = X[i, 0]\n",
    "        # print preds[i]\n",
    "        # print y[i]\n",
    "        tup = (preds[i], y[i])\n",
    "        if pre_qid != -1 and pre_qid != qid:\n",
    "            ndcg = calc_ndcg(vec, at)\n",
    "            ndcg_sum += ndcg\n",
    "            ndcg_num += 1\n",
    "            vec = []\n",
    "        pre_qid = qid\n",
    "        vec.append(tup)\n",
    "    if pre_qid != -1 and pre_qid != qid:\n",
    "        ndcg = calc_ndcg(vec, at)\n",
    "        ndcg_sum += ndcg\n",
    "        ndcg_num += 1\n",
    "\n",
    "    score = ndcg_sum / ndcg_num\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.utils import check_X_y, check_array\n",
    "from xgboost import DMatrix, train\n",
    "from xgboost import XGBModel\n",
    "from xgboost.sklearn import _objective_decorator\n",
    "\n",
    "\n",
    "def _preprare_data_in_groups(X, y=None, sample_weights=None):\n",
    "    \"\"\"\n",
    "    Takes the first column of the feature Matrix X given and\n",
    "    transforms the data into groups accordingly.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : (2d-array like) Feature matrix with the first column the group label\n",
    "\n",
    "    y : (optional, 1d-array like) target values\n",
    "\n",
    "    sample_weights : (optional, 1d-array like) sample weights\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    group_sizes: (1d-array) group sizes\n",
    "\n",
    "    X_features : (2d-array) features sorted per group\n",
    "\n",
    "    y : (None or 1d-array) Target sorted per group\n",
    "\n",
    "    sample_weights: (None or 1d-array) sample weights sorted per group\n",
    "    \"\"\"\n",
    "    group_labels = X[:, 0]\n",
    "    group_indices = group_labels.argsort()\n",
    "    group_labels = group_labels[group_indices]\n",
    "    _, group_sizes = np.unique(group_labels, return_counts=True)\n",
    "    X_sorted = X[group_indices]\n",
    "    X_features = X_sorted[:, 1:]\n",
    "\n",
    "    if y is not None:\n",
    "        y = y[group_indices]\n",
    "\n",
    "    if sample_weights is not None:\n",
    "        sample_weights = sample_weights[group_indices]\n",
    "\n",
    "    return group_sizes,group_indices, X_features, y, sample_weights\n",
    "\n",
    "\n",
    "class XGBRanker(XGBModel):\n",
    "    \"\"\"Implementation of sklearn API for XGBoost Ranking\n",
    "\n",
    "        Parameters\n",
    "    ----------\n",
    "    max_depth : int\n",
    "        Maximum tree depth for base learners.\n",
    "    learning_rate : float\n",
    "        Boosting learning rate (xgb's \"eta\")\n",
    "    n_estimators : int\n",
    "        Number of boosted trees to fit.\n",
    "    silent : boolean\n",
    "        Whether to print messages while running boosting.\n",
    "    n_jobs : int\n",
    "        Number of parallel threads used to run xgboost.  (replaces nthread)\n",
    "    gamma : float\n",
    "        Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "    min_child_weight : int\n",
    "        Minimum sum of instance weight(hessian) needed in a child.\n",
    "    max_delta_step : int\n",
    "        Maximum delta step we allow each tree's weight estimation to be.\n",
    "    subsample : float\n",
    "        Subsample ratio of the training instance.\n",
    "    colsample_bytree : float\n",
    "        Subsample ratio of columns when constructing each tree.\n",
    "    colsample_bylevel : float\n",
    "        Subsample ratio of columns for each split, in each level.\n",
    "    reg_alpha : float (xgb's alpha)\n",
    "        L1 regularization term on weights\n",
    "    reg_lambda : float (xgb's lambda)\n",
    "        L2 regularization term on weights\n",
    "    scale_pos_weight : float\n",
    "        Balancing of positive and negative weights.\n",
    "    base_score:\n",
    "        The initial prediction score of all instances, global bias.\n",
    "    random_state : int\n",
    "        Random number seed.  (replaces seed)\n",
    "    missing : float, optional\n",
    "        Value in the data which needs to be present as a missing value. If\n",
    "        None, defaults to np.nan.\n",
    "    **kwargs : dict, optional\n",
    "        Keyword arguments for XGBoost Booster object.  Full documentation of parameters can\n",
    "        be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.md.\n",
    "        Attempting to set a parameter via the constructor args and **kwargs dict simultaneously\n",
    "        will result in a TypeError.\n",
    "        Note:\n",
    "            **kwargs is unsupported by Sklearn.  We do not guarantee that parameters passed via\n",
    "            this argument will interact properly with Sklearn.\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    A custom objective function can be provided for the ``objective``\n",
    "    parameter. In this case, it should have the signature\n",
    "    ``objective(y_true, y_pred) -> grad, hess``:\n",
    "\n",
    "    y_true: array_like of shape [n_samples]\n",
    "        The target values\n",
    "    y_pred: array_like of shape [n_samples]\n",
    "        The predicted values\n",
    "\n",
    "    grad: array_like of shape [n_samples]\n",
    "        The value of the gradient for each sample point.\n",
    "    hess: array_like of shape [n_samples]\n",
    "        The value of the second derivative for each sample point\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_depth=3, learning_rate=0.1, n_estimators=100,\n",
    "                 silent=True, \n",
    "                 n_jobs=-1, gamma=0, min_child_weight=1, max_delta_step=0,\n",
    "                 subsample=1, colsample_bytree=1, colsample_bylevel=1,\n",
    "                 reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
    "                 base_score=0.5, random_state=0, missing=None, **kwargs):\n",
    "\n",
    "        super(XGBRanker, self).__init__(max_depth, learning_rate,\n",
    "                                        n_estimators, silent, 'rank:pairwise', 'gbtree',\n",
    "                                        n_jobs, None, gamma, min_child_weight, max_delta_step,\n",
    "                                        subsample, colsample_bytree, colsample_bylevel,\n",
    "                                        reg_alpha, reg_lambda, scale_pos_weight,\n",
    "                                        base_score, random_state, None, missing)\n",
    "\n",
    "    def get_xgb_params(self):\n",
    "        \"\"\"\n",
    "        Get xgboost type parameters.\n",
    "        xgb_params is based on the signature of __init__.\n",
    "        in our case, \"objective\" absent in the signature of __init__\n",
    "        so we need to add it.\n",
    "        \"\"\"\n",
    "        xgb_params = self.get_params()\n",
    "\n",
    "        xgb_params['silent'] = 1 if self.silent else 0\n",
    "\n",
    "        if self.nthread <= 0:\n",
    "            xgb_params.pop('nthread', None)\n",
    "                # add default params for ranking\n",
    "        xgb_params[\"objective\"] = self.objective\n",
    "        xgb_params[\"booster\"] = self.booster\n",
    "        return xgb_params\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None, eval_set=None, eval_metric=None,\n",
    "            early_stopping_rounds=None, verbose=True, xgb_model=None):\n",
    "        \"\"\"\n",
    "        Fit the gradient boosting model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array_like\n",
    "            Feature matrix with the first feature containing a group indicator\n",
    "        y : array_like\n",
    "            Labels\n",
    "        sample_weight : array_like\n",
    "            instance weights\n",
    "        eval_set : list, optional\n",
    "            A list of (X, y) tuple pairs to use as a validation set for\n",
    "            early-stopping\n",
    "        eval_metric : str, callable, optional\n",
    "            If a str, should be a built-in evaluation metric to use. See\n",
    "            doc/parameter.md. If callable, a custom evaluation metric. The call\n",
    "            signature is func(y_predicted, y_true) where y_true will be a\n",
    "            DMatrix object such that you may need to call the get_label\n",
    "            method. It must return a str, value pair where the str is a name\n",
    "            for the evaluation and value is the value of the evaluation\n",
    "            function. This objective is always minimized.\n",
    "        early_stopping_rounds : int\n",
    "            Activates early stopping. Validation error needs to decrease at\n",
    "            least every <early_stopping_rounds> round(s) to continue training.\n",
    "            Requires at least one item in evals.  If there's more than one,\n",
    "            will use the last. Returns the model from the last iteration\n",
    "            (not the best one). If early stopping occurs, the model will\n",
    "            have three additional fields: bst.best_score, bst.best_iteration\n",
    "            and bst.best_ntree_limit.\n",
    "            (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
    "            and/or num_class appears in the parameters)\n",
    "        verbose : bool\n",
    "            If `verbose` and an evaluation set is used, writes the evaluation\n",
    "            metric measured on the validation set to stderr.\n",
    "        xgb_model : str\n",
    "            file name of stored xgb model or 'Booster' instance Xgb model to be\n",
    "            loaded before training (allows training continuation).\n",
    "        \"\"\"\n",
    "\n",
    "        X, y = check_X_y(X, y, accept_sparse=False, y_numeric=True)\n",
    "        group_sizes, _, X_features, y, sample_weight = _preprare_data_in_groups(X, y, sample_weight)\n",
    "        params = self.get_xgb_params()\n",
    "        evals_result = {}\n",
    "        feval = eval_metric if callable(eval_metric) else None\n",
    "        if eval_metric is not None:\n",
    "            if callable(eval_metric):\n",
    "                eval_metric = None\n",
    "            else:\n",
    "                params.update({'eval_metric': eval_metric})\n",
    "\n",
    "        if sample_weight is not None:\n",
    "            train_dmatrix = DMatrix(X_features, label=y, weight=sample_weight,\n",
    "                                    missing=self.missing)\n",
    "        else:\n",
    "            train_dmatrix = DMatrix(X_features, label=y,\n",
    "                                    missing=self.missing)\n",
    "        train_dmatrix.set_group(group_sizes)\n",
    "\n",
    "        self._Booster = train(params, train_dmatrix,\n",
    "                              self.n_estimators,\n",
    "                              early_stopping_rounds=early_stopping_rounds,\n",
    "                              evals_result=evals_result, obj=None, feval=feval,\n",
    "                              verbose_eval=verbose, xgb_model=xgb_model)\n",
    "\n",
    "        if evals_result:\n",
    "            for val in evals_result.items():\n",
    "                evals_result_key = list(val[1].keys())[0]\n",
    "                evals_result[val[0]][evals_result_key] = val[1][evals_result_key]\n",
    "            self.evals_result = evals_result\n",
    "\n",
    "        if early_stopping_rounds is not None:\n",
    "            self.best_score = self._Booster.best_score\n",
    "            self.best_iteration = self._Booster.best_iteration\n",
    "            self.best_ntree_limit = self._Booster.best_ntree_limit\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, output_margin=False, ntree_limit=0):\n",
    "        '''\n",
    "        X (array_like) – The dmatrix storing the input.\n",
    "        output_margin (bool) – Whether to output the raw untransformed margin value.\n",
    "        ntree_limit (int) – Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
    "        '''\n",
    "        sizes, group_indices, X_features, _, _ = _preprare_data_in_groups(X)\n",
    "\n",
    "        test_dmatrix = DMatrix(X_features, missing=self.missing)\n",
    "        test_dmatrix.set_group(sizes)\n",
    "        rank_values = self.get_booster().predict(test_dmatrix,\n",
    "                                                 output_margin=output_margin,\n",
    "                                                 ntree_limit=ntree_limit)\n",
    "        revert_group_indices = np.arange(len(group_indices))[group_indices.argsort()]\n",
    "        rank_values = rank_values[revert_group_indices]\n",
    "        return rank_values\n",
    "\n",
    "    def apply(self, X, ntree_limit=0):\n",
    "        \"\"\"Return the predicted leaf every tree for each sample.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array_like, shape=[n_samples, n_features]\n",
    "            Input features matrix.\n",
    "\n",
    "        ntree_limit : int\n",
    "            Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_leaves : array_like, shape=[n_samples, n_trees]\n",
    "            For each datapoint x in X and for each tree, return the index of the\n",
    "            leaf x ends up in. Leaves are numbered within\n",
    "            ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
    "        \"\"\"\n",
    "        sizes, group_indices, X_features, _, _ = _preprare_data_in_groups(X)\n",
    "        test_dmatrix = DMatrix(X_features, missing=self.missing)\n",
    "        test_dmatrix.set_group(sizes)\n",
    "        X_leaves = self.get_booster().predict(test_dmatrix,\n",
    "                                          pred_leaf=True,\n",
    "                                          ntree_limit=ntree_limit)\n",
    "        revert_group_indices = np.arange(len(group_indices))[group_indices.argsort()]\n",
    "        X_leaves = X_leaves[revert_group_indices, :]\n",
    "        return X_leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ranker = XGBRanker(learning_rate=0.1, n_estimators=140, max_depth=8, reg_lambda=1.4, reg_alpha=0.6,\n",
    "                       min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                       n_jobs=-1, scale_pos_weight=1, silent=True, seed=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRanker(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8, gamma=0,\n",
       "     learning_rate=0.1, max_delta_step=0, max_depth=8, min_child_weight=1,\n",
       "     missing=None, n_estimators=140, n_jobs=-1, random_state=0,\n",
       "     reg_alpha=0.6, reg_lambda=1.4, scale_pos_weight=1, silent=True,\n",
       "     subsample=0.8)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker.fit(group_X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.6347931 ,  1.04782224,  0.11335668, ...,  0.10594454,\n",
       "        0.14751428,  0.84518361], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker.predict(group_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[341, 370, 369, ..., 312, 229, 113],\n",
       "       [370, 350, 375, ..., 312, 217, 126],\n",
       "       [249, 272, 294, ..., 197, 166, 113],\n",
       "       ..., \n",
       "       [287, 364, 309, ..., 197, 184, 113],\n",
       "       [254, 263, 309, ..., 197, 165, 101],\n",
       "       [370, 376, 414, ..., 212, 184, 113]], dtype=int32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker.apply(group_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([2,0,3,1,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ai = a.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a[ai]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 0, 2, 4])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = b[ai]\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 3, 1, 4])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aii = np.arange(len(ai))\n",
    "aii[ai.argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = np.arange(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 0, 2, 4])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[a.argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
